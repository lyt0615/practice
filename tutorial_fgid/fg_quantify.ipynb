{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **深度学习应用于拉曼光谱官能团数量的识别**  \n",
    "*Deep Learning for Fuctional Group Quantification Using Raman Spectra.*\n",
    "\n",
    "本课题利用深度学习模型，根据有机分子的拉曼光谱，识别分子中存在的官能团。\n",
    "## 目录"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 配置虚拟环境 \n",
    ">**如果你已经配置过其他拥有torch的环境，激活该环境后跳到(5)即可。**\n",
    "\n",
    "(1) 打开终端 \n",
    " \n",
    "(2) 新建环境  \n",
    "```\n",
    "conda create -n fgid python=3.10\n",
    "```\n",
    "(3) 进入环境\n",
    "```\n",
    "conda activate fgid\n",
    "```\n",
    "(4) 安装所需包\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "(5) 安装rdkit  \n",
    "rdkit是用来对化学分子进行结构展示、性质计算等操作的开源工具包。我们将使用rdkit展示分子结构式。 \n",
    "```\n",
    "pip install rdkit-pypi==2022.9.4\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 数据准备\n",
    "## 2.1 定义随机种子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, torch, random\n",
    "SEED = 42 # 也可以设置别的数，设置后保持不变即可。\n",
    "\n",
    "random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 数据集的构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 补全官能团的SMARTS，参考[rdkit_practice](../rdkit_practice)："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = {\n",
    "        'Alkane':,\n",
    "        'Alkene':,\n",
    "        'Alkyne':,\n",
    "        'Arene':,\n",
    "        'Haloalkane':,\n",
    "        'Alcohol':,\n",
    "        'Aldehyde':,\n",
    "        'Ketone':,\n",
    "        'Ester':,\n",
    "        'Ether':,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 进行数据的提取和清洗，保存为.pkl或.npz文件：    \n",
    "**注意标签的元素不再是0和1，而是官能团的个数**；应该需要把标签归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  # 光谱数据\n",
    "label =  # 标签，1表示官能团存在，0表示不存在。\n",
    "smiles = "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 数据集的划分\n",
    "我们采用分层抽样，将数据集划分成数量比为训练:验证:测试=3:1:1的三个子集，保证每种官能团在各个子集里的数量占比与在全集中相同（实际情况下不可能完全均分，所以取整作近似）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "\n",
    "shuffle_splits = 1\n",
    "# 定义分层抽样器1，使测试集占全集20%。test_size=1/(1+1+3)=0.2\n",
    "str_shuffle_1 = MultilabelStratifiedShuffleSplit(n_splits=shuffle_splits, test_size=0.2, random_state=SEED)\n",
    "# 定义分层抽样器2，使验证集占剩训练+验证数据的25%。test_size=1/(1+3)=0.25\n",
    "str_shuffle_2 = MultilabelStratifiedShuffleSplit(n_splits=shuffle_splits, test_size=0.25, random_state=SEED) \n",
    "\n",
    "for train_val_index, test_index in str_shuffle_1.split(data, label):\n",
    "    X_train_val = data[train_val_index]\n",
    "    X_test = data[test_index]\n",
    "    y_train_val = label[train_val_index]\n",
    "    y_test = label[test_index]\n",
    "    smiles_train_val = smiles[train_val_index]\n",
    "    smiles_test = smiles[test_index]\n",
    "for train_index, val_index in str_shuffle_2.split(X_train_val, y_train_val):\n",
    "    X_train = X_train_val[train_index]\n",
    "    X_val = X_train_val[val_index]\n",
    "    y_train = y_train_val[train_index]\n",
    "    y_val = y_train_val[val_index]\n",
    "    smiles_train = smiles_train_val[train_index]\n",
    "    smiles_val = smiles_train_val[val_index]\n",
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 构建Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "def get_dataloader(X, y, batch_size=64):\n",
    "    '''\n",
    "    将光谱X和标签y配对并分为len(X)/batch_size个批次，每批次batch_size个光谱-标签对。'''\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.float32)\n",
    "    dataset = TensorDataset(X, y)\n",
    "    # weighted_sampler = sampler.WeightedRandomSampler(sampling_weight, len(sampling_weight))\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    return dataloader\n",
    "\n",
    "train_loader = get_dataloader(X_train, y_train, batch_size=128)\n",
    "val_loader = get_dataloader(X_val, y_val, batch_size=64)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 训练模型\n",
    "## 3.1 训练组件设置\n",
    "(1) 使用gpu加速  \n",
    ">如果下面代码输出cpu但是你的电脑有gpu，可以参考教程：[选择正确版本的CUDA和PyTorch安装](https://zhuanlan.zhihu.com/p/672526561)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device # 查看在使用的处理器"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) 损失函数    \n",
    "这是一个回归任务，用什么损失函数合适呢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion =\n",
    "optimizer = Adam(params=model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) 早停机制  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, save_path, patience=5, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            save_path: 模型参数保存路径.\n",
    "            patience (int): 监测指标在连续patience次迭代不升高后，自动停止训练。\n",
    "                            Default: 5\n",
    "            delta (float): 监测指标增量小于该值时，不保存模型。\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.save_path = save_path\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} / {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        path = os.path.join(self.save_path, 'best_network.pth')\n",
    "        torch.save(model.state_dict(), path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 设置训练参数和模型初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import CNN\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "\n",
    "num_epochs = 100 # 最大迭代次数\n",
    "model_save_path = 'model_path' # 保存模型参数的路径\n",
    "learning_rate = 2.5e-4 # 学习率\n",
    "val_threshold = 0.5 # 该阈值用来与模型计算的官能团存在概率作比较，若概率值大于该阈值，认为官能团存在，输出标签的相应维度上元素输出为1，否则为0。\n",
    "model = CNN(class_num=y_train.shape[1]).to(device)\n",
    "'''\n",
    "pos_weight=? 为每类官能团的loss赋予权重，是否可以提高模型性能？'''\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 训练和验证   \n",
    "这是一个回归任务，验证和训练的方式也要改变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_list = []\n",
    "train_loss_list = []\n",
    "val_loss_min = 1\n",
    "\n",
    "earlystopping = EarlyStopping(save_path=model_save_path)\n",
    "val_acc_max = 0.5\n",
    "\n",
    "time_start = time()\n",
    "for epoch in range(1, num_epochs+1):\n",
    "\n",
    "# 训练\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    train_bar = tqdm(enumerate(train_loader), total=len(train_loader), ncols=50)\n",
    "    for _, (batch_x, batch_y) in train_bar:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x.to(device))\n",
    "        loss = criterion(outputs, batch_y.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch}/{num_epochs}], Training Loss: {train_loss}.')\n",
    "\n",
    "# 验证\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    num_correct = 0\n",
    "    with torch.no_grad():\n",
    "        val_bar = tqdm(enumerate(val_loader), total=len(val_loader), ncols=50)  # 创建进度条\n",
    "        for _, (batch_x, batch_y) in val_bar:\n",
    "            val_outputs = model(batch_x.to(device))\n",
    "            loss = criterion(val_outputs, batch_y.to(device))\n",
    "            optimizer.zero_grad()\n",
    "            val_loss += loss.item()\n",
    "            '''下面改成什么呢\n",
    "            '''\n",
    "            # predicted_labels = (val_outputs >= val_threshold).float().cpu()\n",
    "            # num_correct += np.count_nonzero(predicted_labels == batch_y)\n",
    "\n",
    "    # val_acc = num_correct/(y_val.shape[0]*y_val.shape[1]) \n",
    "    # val_loss = val_loss / len(val_loader)\n",
    "    # print(f'Epoch [{epoch }/{num_epochs}], Validation Accuracy: {val_acc}.')\n",
    "\n",
    "    if earlystopping:\n",
    "        earlystopping(val_loss, model)\n",
    "        if earlystopping.early_stop:\n",
    "            print(\"Early stopped.\")\n",
    "            time_end=time()\n",
    "            break\n",
    "print(f'Total training time: {(time_end-time_start)//60} mins.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 测试\n",
    "## 4.1 载入模型参数和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(class_num=y_test.shape[1])\n",
    "model.load_state_dict(torch.load('model_path\\\\best_network.pth', map_location=device),\n",
    "                                                          strict=False)\n",
    "test_loader = get_dataloader(X_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bar = tqdm(enumerate(test_loader), total=len(test_loader), ncols=50)  # 创建进度条\n",
    "num_correct\n",
    "y_predicted = []\n",
    "for _, (batch_x, batch_y) in test_bar:\n",
    "    test_outputs = model(batch_x.to(device))\n",
    "    loss = criterion(test_outputs, batch_y.to(device))\n",
    "    optimizer.zero_grad()\n",
    "    # predicted_labels = (test_outputs >= val_threshold).float().cpu().numpy()\n",
    "    # num_correct += np.count_nonzero(predicted_labels == batch_y)\n",
    "    y_predicted.append(predicted_labels)\n",
    "y_predicted = np.vstack(y_predicted)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 模型性能可视化  \n",
    "**性能指标：**  \n",
    "我目前只想到了误差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alexnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
